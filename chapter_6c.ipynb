{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "\n",
    "Create them by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.accumulators.Accumulator"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD = sc.parallelize((1,2,3,4),2)\n",
    "print(testRDD.collect())\n",
    "acc = sc.accumulator(0)\n",
    "type(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def doStuff(x):\n",
    "    global acc #important keyword!!\n",
    "    acc +=1\n",
    "    return x\n",
    "\n",
    "result = testRDD.map(doStuff).collect()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add to them calling the add function in Java or using the += syntax.\n",
    "\n",
    "For the driver to access the value of the accumulator, call <code>acc.value()</code> in Java and <code>acc.value</code> in Sacla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that an action needs to be called first... (e.g. <code>collect()</code> above).\n",
    "\n",
    "Accumulators can be thought of WRITE ONLY variables. Results can be written back to the Driver but cannot be accessed from the worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "Accumulators inside TRANSFORMATIONS are impacted by dying or slow nodes which can lead to duplicate computations.\n",
    "\n",
    "Accumulators inside ACTIONS are only updated once and therefor accurate. \n",
    "\n",
    "==> Accumulators in transformations should be used for debugging purposes only!\n",
    "\n",
    "### Custom Accumulators\n",
    "\n",
    "Besides integers, Spark supports Double, Float and Long types for accumulators. Custom accumulators must extend <code> AccumulatorParam</code>. In particular, operations need to be commutative and associative (i.e. a OP b = b OP a  and   (A OP b) OP C = A OP (B OP C)). This includes sum and max. \n",
    "\n",
    "# Broadcast Variables\n",
    "\n",
    "Broadcast variables can be thought of READ ONLY variables. A typical use case would be if your application needs to send a large, read-only lookup table to all the nodes (or a large feature vector in a ML program).\n",
    "\n",
    "Broadcast variables must be serializable. The variable will be sent to each node only once. Updates will not be propagated to other nodes (==> write read only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('+1', 5), ('+49', 3), ('+1', 2), ('+49', 31)]\n",
      "{'+49': 'Germany', '+1': 'USA'}\n",
      "[('USA', 7), ('Germany', 34)]\n"
     ]
    }
   ],
   "source": [
    "contactCounts = sc.parallelize((('+1',5), ('+49',3),('+1',2),('+49',31)))\n",
    "print(contactCounts.collect())\n",
    "\n",
    "# WITHOUT BRAODCASTING\n",
    "signPrefixes = sc.parallelize([['+1','USA'],['+49','Germany']]).collectAsMap() \n",
    "print(signPrefixes)\n",
    "\n",
    "def processSignCount(sign_count):\n",
    "    country = signPrefixes[sign_count[0]]\n",
    "    count = sign_count[1]\n",
    "    return (country,count)\n",
    "countryContactCounts = (contactCounts.map(processSignCount).reduceByKey(lambda x,y:x+y))\n",
    "print(countryContactCounts.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'>\n",
      "[('USA', 7), ('Germany', 34)]\n"
     ]
    }
   ],
   "source": [
    "# WITH BROADCASTING\n",
    "signPrefixes = sc.parallelize([['+1','USA'],['+49','Germany']]).collectAsMap() \n",
    "signPrefixes = sc.broadcast(signPrefixes) # NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW\n",
    "print(type(signPrefixes))\n",
    "\n",
    "def processSignCount(sign_count):\n",
    "    country = signPrefixes.value[sign_count[0]] # NEW NEW NEW NEW NEW NEW NEW NEW\n",
    "    count = sign_count[1]\n",
    "    return (country,count)\n",
    "countryContactCounts = (contactCounts.map(processSignCount).reduceByKey(lambda x,y:x+y))\n",
    "print(countryContactCounts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables are particularly useful when the variable (lookup table etc.) is LARGE (Sparks default task launcher is optimized for small tasks!) and used in MULTIPLE PARALLEL OPERATIONS. \n",
    "\n",
    "Spark uses the Java Serializer when when sending variables over the network. This can be slow for anything other than arrays of primitive types. Alternatively, one can select a different serializer by changing the spark.serializer property or building your custom serializer.\n",
    "\n",
    "# Working on a Per-Partition Basis \n",
    "## with <code>mapPartitions()</code>, <code>mapPartitionsWithIndex()</code>, <code>foreachPartition()</code>\n",
    "\n",
    "<code>mapPartitions()</code> can be used as an alternative to <code>map()</code> & <code>foreach()</code>. <code>mapPartitions()</code> is called ONCE FOR EACH PARTITION unlike <code>map()</code> & <code>foreach()</code> which is called for each element in the RDD. The main advantage being that, we can do initialization on Per-Partition basis instead of per-element basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "['1 -> 0', '2 -> 1', '3 -> 1', '4 -> 2', '5 -> 2']\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 3\n",
    "rdd = sc.parallelize([1,2,3,4,5],numPartitions)\n",
    "print(rdd.collect())\n",
    "\n",
    "def mapToPartitionIndex(index,iterator):\n",
    "    return [str(x) + \" -> \" + str(index) for x in iterator]\n",
    "\n",
    "mapped =   rdd.mapPartitionsWithIndex(mapToPartitionIndex)\n",
    "\n",
    "print(mapped.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "(15, 5)\n",
      "result:  3.0\n",
      " \n",
      "without creating (num,1) tuples:\n",
      "results on partitions:  [[1, 1], [5, 2], [9, 2]]\n",
      "accumulated result:  (15, 5)\n",
      "result:  3.0\n"
     ]
    }
   ],
   "source": [
    "#Averaging without mapPartitions()\n",
    "def combineCtrs(c1,c2):\n",
    "    return (c1[0] +c2[0],c1[1] + c2[1])\n",
    "def basicAvg(nums):\n",
    "    '''compute the average'''\n",
    "    result = nums.map(lambda num: (num,1)).reduce(combineCtrs)\n",
    "    return result[0]/float(result[1])\n",
    "\n",
    "print(rdd.map(lambda num: (num,1)).collect())\n",
    "print(rdd.map(lambda num: (num,1)).reduce(combineCtrs))\n",
    "print(\"result: \",basicAvg(rdd))\n",
    "print(\" \")\n",
    "\n",
    "#Averaging with mapPartitions()\n",
    "def partitionCtr(nums):\n",
    "    '''cumpute sumCOunter for partition'''\n",
    "    sumCount = [0,0]\n",
    "    for num in nums:\n",
    "        sumCount[0] += num\n",
    "        sumCount[1] += 1\n",
    "    return [sumCount]\n",
    "def fastAvg(nums):\n",
    "    '''compute the average'''\n",
    "    sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
    "    return sumCount[0] / float(sumCount[1])\n",
    "\n",
    "print('without creating (num,1) tuples:')\n",
    "print(\"results on partitions: \",rdd.mapPartitions(partitionCtr).collect())\n",
    "print(\"accumulated result: \",rdd.mapPartitions(partitionCtr).reduce(combineCtrs)) #compare to result from mapToPartitionIndex\n",
    "print(\"result: \",fastAvg(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piping to External Programs\n",
    "\n",
    "<code>pipe()</code> returns an RDD created by piping elements to a forked external process. The process needs read and write to Unix standard streams. ==> <code>pipe()</code> is transformation of a RDD that reads each element from standard input as a <code>String</code> and writes results to standard output as <code>Strings</code>.\n",
    "\n",
    "\n",
    "echo.bat: \n",
    "<code>\n",
    "@echo off\n",
    "echo \"stupid batch\"\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"stupid batch\"\\r', '\"stupid batch\"\\r']\n",
      "['\"stupid batch\"\\r', '\"stupid batch\"\\r', '\"stupid batch\"\\r']\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize(['1', '2', '', '3'],2).pipe('echo.bat').collect())\n",
    "print(sc.parallelize(['1', '2', '', '3'],3).pipe('echo.bat').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..gets executed on every executor\n",
    "what about R files? how does spark know how to execute R (perl, fortran,...) code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric RDD Operations\n",
    "\n",
    "Spark provides descriptive statistics on numeric RDDs. THey are implemented with a streaming algorithm that allows for building up our model one element at a time. In a single pass over the data they are returned as a <code>StatsCounter</code> object by calling <code>stats()</code>. Available are:\n",
    "\n",
    "<code> count(), mean(), sum(), max(), min(), varaince(), sampleVariance(), stdev(), sampleStdev()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "max:  5\n",
      "mean:  3.0\n",
      "stats:  (count: 5, mean: 3.0, stdev: 1.41421356237, max: 5.0, min: 1.0)\n",
      "count from StatsCounter:  5\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())\n",
    "print(\"max: \", rdd.max())\n",
    "print(\"mean: \", rdd.mean())\n",
    "\n",
    "stats = rdd.stats()\n",
    "print(\"stats: \",stats)\n",
    "print(\"count from StatsCounter: \", stats.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
