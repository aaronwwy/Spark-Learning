{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs\n",
    "## RDD Basics\n",
    "* RDD: An immutable distributed collection of objects. Each split into multiple partitions, which pay be computed on different nodes of the cluster\n",
    "* Created in 2 ways:\n",
    "    1. Loading an external dataset\n",
    "    2. Distributing a collection of objects (e.g. list or set) in their driver program\n",
    "* RDDs offer 2 types of operations:\n",
    "    1. Transformations: construct a new RDD from a previous one\n",
    "    2. Actions: Compute a result based on an RDD, either return it to the driver program or save it to an external storage system (e.g. HDFS)\n",
    "* Difference: Although you can define new RDDs any time, Spark only computes them in a <i> lazy</i> fashion, the first time they are used in an action\n",
    "* RDDs are by default recomputed each time you run an action on them\n",
    "    - If you would like to reuse RDD in multiple actions, you can ask Spark to <i>persist</i> it using <code>RDD.persist()</code>. to store the RDD contents in memory (partitioned across the machines in your cluster) and reuse them in future actions\n",
    "    - It does not persist by default because in the context of big data if you do not use the RDD, there is no reason to waste storage space when Spark could instead stream through the data once and just compute the result\n",
    "\n",
    "### Example: Filter data that matches a predicate\n",
    "Create a new RDD holding just the strings that contain \"Python\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines: 95\n",
      "First line: # Apache Spark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:25: error: not found: value pythonLines\n",
       "val $ires14 = pythonLines\n",
       "              ^\n",
       "<console>:22: error: not found: value pythonLines\n",
       "         pythonLines = lines.filter(line => line.contains(\"Python\"))\n",
       "         ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"README_spark.md\") \n",
    "println(\"number of lines: \" + lines.count())\n",
    "println(\"First line: \" + lines.first())\n",
    "pythonLines = lines.filter(line => line.contains(\"Python\"))\n",
    "pythonLines.first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
