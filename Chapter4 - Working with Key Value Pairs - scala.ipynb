{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs\n",
    "\n",
    "## Pair RDDs\n",
    "* Spark provides special operations on RDDs containing key/value pairs - these RDDs are called pair RDDs\n",
    "* We can use the <code>map()</code> function to create a pair RDD as shown below\n",
    "* Pair RDDs are still RDDs (of Tuple2 objects in Java/Scala or of Python tuples) \n",
    "\n",
    "### Example: Creating a pair RDD using the first word as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from map: \n",
      "(his,his name is pat)\n",
      "(his,his name is peter)\n",
      "(his,his name is olaf)\n",
      "(her,her name is Joanne)\n",
      "(her,her name is Therese)\n",
      "(they,they work at algebraix in Encintas )\n",
      "(they,they like to eat yogurt )\n",
      "\n",
      "Results from lookup: \n",
      "her name is Joanne\n",
      "her name is Therese\n"
     ]
    }
   ],
   "source": [
    "val lines = sc.textFile(\"name_example.txt\")\n",
    "val rddpair = lines.map(x => (x.split(\" \")(0),x))\n",
    "\n",
    "\n",
    "println(\"Results from map: \")\n",
    "rddpair.collect().foreach(println)\n",
    "\n",
    "println(\"\\nResults from lookup: \")\n",
    "rddpair.lookup(\"her\").foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Pair RDDs\n",
    "\n",
    "### Transformation on one pair RDD\n",
    "* Pair RDDs are allowed to use all the transformations available to standard RDDs\n",
    "* The same rules apply from \"Passing Functions to Spark\" from Chapter 3\n",
    "    - Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on individual elements\n",
    "* <code>reduceByKey(func)</code>: Combine values with the same key \n",
    "    - Note that calling <code>reduceByKey()</code> and <code>foldByKey()</code> will automatically perform combining locally on each machine before computing global totals for each key and hte user does not need to specify a combiner\n",
    "    - The more general <code>combineByKey</code> interface allows you to customize combining behavior\n",
    "    \n",
    "* <code>groupByKey()</code>: Group values with the same key\n",
    "* <code>combineByKey(createCombiner,mergeValue,mergeCombiners,partitioner)</code>: Combine values with the same key using a different result type\n",
    "    - <code>combineByKey()</code> is the most general of the per-key aggregation functions\n",
    "    - Most of the other per-key combiners are implemented using it\n",
    "    - Like <code>aggregate</code>, <code>combineByKey()</code> allows the user to return values that are not hte same type as our input data \n",
    "    - Understsanding <code>combineByKey()</code> in depth:<br>\n",
    "    For each element in the partition, the element either has a key it hasn't seen before or has the same key as a previous element:\n",
    "        - If it's a new element, <code>combineByKey()</code> uses a function we provide, called <code>createCombiner()</code> to create the initial value for the accumulator on the key (**Note: This happens the first time a key is found in each partition rather than only the first time the key is found in the RDD**)\n",
    "        - If it is a value we have seen before while processing that partition, it will instead use the provided function, <code>mergeValue()</code>, with the current value for the accumulator for that key and the new value\n",
    "        - <code>mergeCombiners()</code>: merges the accumulators of the same key across all partitions\n",
    "* <code>mapValues(func)</code>: Apply a function to each value of a pair RDD without changing they key\n",
    "* <code>flatMapValues(func)</code>: Apply a function taht returns an iterator to each value of a pair RDD, and for each element returned produce a key/value entry with the old key. Often used for tokenization \n",
    "* <code>keys()</code>: Return an RDD of just the keys\n",
    "* <code>values()</code>: Return an RDD of just the values\n",
    "* <code>sortByKey()</code>: Return an RDD sorted by the key\n",
    "\n",
    "### Transformation on two pair RDDs\n",
    "* <code> rdd.subtractByKey(other)</code>: Remove elements with a key present in the other RDD\n",
    "* <code> rdd.join(other)</code>: Perform an inner join between two RDDs\n",
    "* <code> rdd.rightOuterJoin(other)</code>: Perform a join between two RDDs where the key must be present in the first RDD \n",
    "* <code> rdd.leftOuterJoin(other)</code>: Perform a join between two RDDs where the key must be present in the other RDD\n",
    "* <code> rdd.cogroup(other)</code>: Group data from both RDDs sharing the same key. \n",
    "    - <code>cogroup()</code> over two RDDs sharing the same key type, K, with the respective value types V and W returns <code>RDD[(K,(Iterable[V]),Iterable[W]))]</code>\n",
    "    - If one of the RDDs doesn't have elements for a given key that is present in the other RDD, the corresponding <code>Iterable</code> is simply empty\n",
    "    - can be used to implement intersection by key\n",
    "    - can work on three or more RDDs at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on one pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: (3,2),(1,4),(3,6),(3,1)\n",
      "reduceByKey example: (1,4),(3,9)\n",
      "groupByKey example: (1,List(4)),(3,List(2, 6, 1))\n",
      "mapValues example: (3,3),(1,5),(3,7),(3,2)\n",
      "flatMapValues example:\n",
      "\tbefore flatMapValues:(3,Range(2, 3)),(1,Range()),(3,Range()),(3,Range(1, 2, 3))\n",
      "\tafter flatMapValues: (3,2),(3,3),(3,1),(3,2),(3,3)\n",
      "keys example: 3,1,3,3\n",
      "values example: 2,4,6,1\n",
      "sortByKey example: (1,4),(3,2),(3,6),(3,1)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((3,2),(1,4),(3,6),(3,1)))\n",
    "println(\"rdd: \" + rdd.collect().mkString(\",\"))\n",
    "\n",
    "//add values with the same key \n",
    "println(\"reduceByKey example: \" + rdd.reduceByKey(_ + _).collect().mkString(\",\"))\n",
    "\n",
    "//groupByKey() returns an iterable in the values, the mapValues function below converts the iterable to a list\n",
    "println(\"groupByKey example: \" + rdd.groupByKey().mapValues(x => x.toList).collect().mkString(\",\"))\n",
    "\n",
    "//mapValues example\n",
    "println(\"mapValues example: \" + rdd.mapValues(_ + 1).collect().mkString(\",\"))\n",
    "\n",
    "//flatMap example:\n",
    "// rdd.flatMapValues(x => (x to 3)).collect()\n",
    "println(\"flatMapValues example:\")\n",
    "println(\"\\tbefore flatMapValues:\" + rdd.mapValues(x=>(x to 3)).collect().mkString(\",\"))\n",
    "println(\"\\tafter flatMapValues: \" + rdd.flatMapValues(x=>(x to 3)).collect().mkString(\",\"))\n",
    "println(\"keys example: \" + rdd.keys.collect().mkString(\",\"))\n",
    "println(\"values example: \" + rdd.values.collect().mkString(\",\"))\n",
    "println(\"sortByKey example: \" + rdd.sortByKey().collect().mkString(\",\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Actions Available on Pair RDDs\n",
    "* <code>countByKey()</code>: Count the number of elements for each key\n",
    "* <code>collectAsMap()</code>: Collect the result as a map to provide easy lookup\n",
    "* <code>lookup(key)</code>: Return all values associated with the provided key\n",
    "\n",
    "### Examples of Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example rdd: (1,2),(3,4),(3,6)\n",
      "countByKey result: Map(1 -> 1, 3 -> 2)\n",
      "lookup result: WrappedArray(4, 6)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1,2),(3,4),(3,6)))\n",
    "println(\"example rdd: \" + rdd.collect().mkString(\",\"))\n",
    "println(\"countByKey result: \" + rdd.countByKey())\n",
    "println(\"lookup result: \" + rdd.lookup(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey Example\n",
    "\n",
    "<code>combineByKey(createCombiner, mergeValue,mergeCombiners,partitioner)</code>: Combine values with the same key using a different result type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddpair_vehicles.txt file content: \n",
      "car\t1\n",
      "car\t2\n",
      "car\t100\n",
      "taxi\t1\n",
      "taxi\t47\n",
      "bus\t250\n",
      "bus\t26\n",
      "uber\t10\n",
      "uber\t17\n",
      "results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map(taxi -> (48,2), uber -> (27,2), bus -> (276,2), car -> (103,3))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"rddpair_vehicles.txt\")\n",
    "println(\"rddpair_vehicles.txt file content: \")\n",
    "lines.collect().foreach(println)\n",
    "\n",
    "val rddpair = lines.map(x => (x.split(\"\\t\")(0),x.split(\"\\t\")(1).toInt))\n",
    "val result = rddpair.combineByKey(x => (x,1),(x:(Int,Int),y) => (x._1 + y, x._2 + 1),\n",
    "                                    (x: (Int,Int),y:(Int,Int)) => (x._1 + y._1, x._2 + y._2))\n",
    "println(\"results:\")\n",
    "result.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\t1\n",
      "car\t2\n",
      "car\t3\n",
      "car\t4\n",
      "taxi\t1\n",
      "taxi\t47\n",
      "bus\t250\n",
      "uber\t10\n",
      "uber\t17\n",
      "\n",
      "results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map(taxi -> 48, uber -> 27, bus -> 250, car -> 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines2 = sc.textFile(\"rddpair_vehicles2.txt\")\n",
    "lines2.collect().foreach(println)\n",
    "val rddpair = lines2.map(x => (x.split(\"\\t\")(0),x.split(\"\\t\")(1).toInt))\n",
    "\n",
    "println(\"\\nresults:\")\n",
    "rddpair.reduceByKey((x,y) => x+y).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-key average with <code>reduceByKey()</code> and <code>mapValues()</code> Example\n",
    "<b> Note:</b> In Chapter 3 it was pointed out that <code>map()'s</code> return type does not have to be the same as the input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate result:(pirate,(3,1)),(panda,(1,2)),(pink,(7,2))\n",
      "final result: Map(pirate -> 3.0, pink -> 3.5, panda -> 0.5)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"panda\",0),(\"pink\",3),(\"pirate\",3),(\"panda\",1),(\"pink\",4)))\n",
    "val inter = rdd.mapValues(x => (x,1)).reduceByKey((x,y) => (x._1+y._1,x._2+y._2))\n",
    "println(\"intermediate result:\" + inter.collect().mkString(\",\")) \n",
    "println(\"final result: \" + inter.mapValues(x => x._1.toDouble/x._2).collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Example\n",
    "The word count example below takes the following steps:\n",
    "1. Split each line from the file into words\n",
    "2. Map each word to a tuple containing the word and an initial count of 1 \n",
    "3. Sum up the count for each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of times Spark appears in README_spark.md: 13\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"README_spark.md\")\n",
    "val result = rdd.flatMap(x=>x.split(\" \")).map(x=> (x,1)).reduceByKey((x,y) => (x+y))\n",
    "println(\"number of times Spark appears in README_spark.md: \" + result.lookup(\"Spark\").mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning (Advanced)\n",
    "* In a distributed program, communication is very expensive so laying out the data to minimize network traffic can greatly improve performance\n",
    "* Partitioning is only useful when a dataset is reused <b>multiple</b> times in key-oriented operations such joins\n",
    "* Spark's partitioning is available on all RDDs of key/value pairs and causes the system to group elements based on a function of each key \n",
    "* Ensures that a set of keys will appear together on some node\n",
    "* Example:\n",
    "    - You might choose to hash-partition an RDD to 100 partitions so that keys that have the same hash value modulo 100 appear on the same node\n",
    "    - You might range-partition the RDD into sorted ranges of keys so that the elements with keys in the same range appear on the same node\n",
    "* Many other Spark operations automatically result in an RDD with known partitioning information and many operations other than <code>join()</code> will take advantage of this information\n",
    "    - For example, <code>sortByKey()</code> and <code>groupByKey</code> will result in range-partitioned and hash-partitioned RDDs, respectively\n",
    "    - On the other hand, operations like <code>map()</code> can cause the new RDD to forget the parent's partitioning information because such operations could theoretically modify the key of each record\n",
    "    - Spark's Java and Python APIs benefit from partitioning in the same way as the scala API. However, in Python you pass a number of partitions desired (e.g. <code>rdd.partitionBy(100)</code>)\n",
    "    \n",
    "### Scala Simple Application Example\n",
    "* Consider an aplication that keeps a large table of user information in memory - say, an RDD of <code>(UserID,UserInfo)</code> pairs, where <code>UserInfo</code> contains a list of topics the user is subscribed to \n",
    "* The application periodically combines this table with a smaller file representing events that happen in the past five minutes - say, a table of <code>(UserID, LinkInfo)</code> pairs for users who have clicked a link on a website in those five minutes\n",
    "* We may want to count how many users visited a link that was <b>not</b> one of their subscribed topics\n",
    "* Initialization code: we load the user info from a Hadoop SequenceFile on HDFS\n",
    "* This distributes the elements of <code>userData</code> by the HDFS block where they are found and <b>doesn't provide Spark with any way of knowing which partition a particular <code>UserID</code> is located</b>\n",
    "\n",
    "        val sc = new SparkContext(...)\n",
    "        val userData = sc.sequenceFile[UserID, UserInfo](\"hdfs://...\").persist()\n",
    "        \n",
    "* A Function is called peroidically to process a logfile of events in the past 5 minutes\n",
    "* We assume that this is a SequenceFile containing (<code>userID, LinkInfo</code>) pairs \n",
    "\n",
    "        def processNewLogs(logFileName: String) {\n",
    "            val events = sc.sequenceFile[UserID, LinkInfo](logFileName)\n",
    "            val joined = userData.join(events) //RDD of (UserID, (UserInfo, LinkInfo)) pairs\n",
    "            val offTopicVisits = joined.filter {\n",
    "                case (userID, (userInfo, linkInfo)) => //Expand the tuple into its components\n",
    "                    !userInfo.topics.contains(linkInfo.topic)\n",
    "                    }.count()\n",
    "                    println(\"Number of Visits to non-subscribed topics: \" + offTopicVisits)\n",
    "        }\n",
    "        \n",
    "* This is inefficient because the <code>join()</code> operation is called each time <code>processNewLogs()</code> is invoked, but does not know anything about how the keys are partitioned in the datasets\n",
    "* By default, this operation will hash all the keys of both datasets sending elements with the same key across the network to the same machine and then joining together the elements with the same key as shown below\n",
    "\n",
    "<img src='figures/fig4_4.png' alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "* Fixing this is simple: just use the <code>partitionBy()</code> transformation on <code>userData</code> to hash-partition it at the start of the program \n",
    "* Do this by passing <code>spark.HashPartitioner to partitionBy:</code>\n",
    "        val sc = new SparkContext(...)\n",
    "        val userData = sc.sequenceFile[UserID, UserInfo](\"hdfs://...\")\n",
    "                            .partitionBy(new HashPartitioner(100)) //create 100 Partitions\n",
    "                            .persist()\n",
    "* The <code>processNewLogs()</code> method can remain unchanged\n",
    "* Because we called <code>partitionBy()</code> when building <code>userData</code>, Spark will now know that it is hash-partitioned and calls to <code>join()</code> on it will take advantage of this information\n",
    "    - When we call <code>userData.join(events)</code> Spark will shuffle only the events RDD, sending events with each particular <code>UserID</code> to the machine that contains the corresponding hash partition of <code>userData</code> as shown below:\n",
    "    \n",
    "<img src=\"figures/fig4_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "* The result is that a lot less data is communicated over the network, and the program runs significantly faster\n",
    "* Note: <code> partitionBy()</code> is a transformation, so it always returns a new RDD - it does not change the original RDD in place. Therefore it is important to persist and save as <code>userData</code> the result of <code>partitionBy()</code>, not the original <code>sequenceFile()</code>\n",
    "* In general make the number of partitions at least as large as the number of cores in your cluster\n",
    "    - Failure to persist an RDD after it has been transformed with <code>partitionBy()</code> will cause subsequent uses of the RDD to repeat the partitioning of the data\n",
    "    - This would negate the advantage of <code>partitionBy()</code> resulting in repeated partitioning and shuffling of the data across the network, similar to what occurs without any specificied partitioner \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining an RDD's Partitioner \n",
    "* In Scala and Java you can determine how an RDD is partiioned using its <code>partitioner</code> property (or <code>partitioner()</code> method in Java)\n",
    "    - Returns a <code>scala.Option</code> object which is a Scala class for a container that may or may not contain one item \n",
    "    - Call <code>isDefined()</code> on the <code>Option</code> to check whether it has a value\n",
    "    - Call <code>get()</code> to get this value\n",
    "    - If present, the value will be a <code>spark.Partitioner</code> object \n",
    "    - This is essentially a function telling the RDD which partition each key goes into\n",
    "* Example: Determining the partitioner of an RDD:\n",
    "        scala> val pairs = sc.parallelize(List((1,1), (2,2), (3,3))) //create an RDD with no partitioning \n",
    "        pairs: spark.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:12\n",
    "        \n",
    "        scala>pairs.partitioner\n",
    "        res0: Option[spark.Partitioner] = None //partitioner validates no partitioning \n",
    "        \n",
    "        //create new RDD w/ partitioning\n",
    "        scala> val partitioned = pairs.partitionBy(new spark.HashPartitioner(2)) \n",
    "                                        .persist() // to actually use partitioned in further operations \n",
    "        partitioned: spark.RDD[(Int, Int)] = ShuffledRDD[1] at partitionBy at <console>:14\n",
    "        \n",
    "        scala> partitioned.partitioner \n",
    "        res1: Option[spark.Partitioner] = Some(spark.HashPartitioner@5147788d) //validates the partitioning \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Level of Parallelism\n",
    "* Spark will always try to infer a sensible default number of partitions based on te size of your cluster, but in some cases you will want to tune hte level of parallelism for better performance \n",
    "* <code>repartition()</code>: shuffles the data across the network to create a new set of partitions\n",
    "    - Note: repartitioning your data is a fairly expensive operation \n",
    "* <code>coalesce()</code>: an optimized version of <code>repartition()</code> that allows avoiding data movement, but only if you are decreasing the number of RDD partitions\n",
    "    - To know whether you can safely call <code>coalesce()</code>, you can check the size of the RDD using <code>rdd.partitions.size()</code> (Java/Scala) and <code>rdd.getNumPartitions()</code> (Python) to make sure you are coalescing it to fewer partitions than it currently has "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations That Benefit from Partitioning\n",
    "* Operations that benefit from parittioning: <code>cogroup()</code>, <code>grouWith()</code>, <code>join()</code>,<code>leftOuterJoin()</code>,<code>rightOuterJoin()</code>,<code>groupByKey()</code>, <code>reduceByKey()</code>, <code>comebineByKey()</code>,<code>lookup()</code>\n",
    "    - All these functions do not change the partitions because they only operate on the values\n",
    "* For operations that act on a single RDD, such as <code>reduceByKey()</code>, running on a prepartioned RDD will cause all the values for each key to be computed locally on a single machine requring only the final local reduce value to be sent from each worker node by to the master\n",
    "\n",
    "## Operations That Affect Partitioning\n",
    "* Spark knows internally how each of its operators affects partitioning and automatically sets the <code>partitioner</code> on RDDs created by operations that partition the data \n",
    "* Suppose you call <code>join()</code> to join two RDDs; because the elements with the same key have been hased to the same machine, Spark knows that the result is hash-partitioned and operations like <code>reduceByKey()</code> on te join result are going to be signficantly faster \n",
    "* Flipside: for transformations that cannot be guaranteed to produce a known partitioning, the output RDD will not have a partitioner set\n",
    "    - Example: if you call <code>map()</code> on a hash-partitioned RDD of key/value pairs, the function passed to <code>map()</code> can in theory change the key of each elemen, so the result will not have a <code>partitioner</code> set \n",
    "* Operations such as <code>mapValues()</code> and <code>flatMapValues()</codE> guarantees that each tuple's key remains the same so use that shit instead if you don't intend to change the key\n",
    "* Operations that result in a partitioner being set on the output RDD:\n",
    "    - <code>cogroup(), groupWith(), join(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey(), partitionBy(), sort()</code>\n",
    "    - If the parent has a partiioner: <code>mapValues(), flatMapValues(), filter()</code>\n",
    "* For binary operations <i>which</i> partitioner is set on the output depends on the parent RDDs' partitioners\n",
    "* By default, it is a hash partitioner, with the number of partitions set to the level of parallelism of the operation\n",
    "    - Set to whatever the operation decides to set it as \n",
    "* If one of the parents has a <code>partitioner</code> set, it will be that partitioner\n",
    "* If both parents have a <code>partitioner</code> set, it will be the partioner of the first parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//page rank example skipped - come back to this later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
