{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The below cell generates the table of contents - don't mess with it *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs\n",
    "## RDD Basics\n",
    "* RDD: An immutable distributed collection of objects. Each split into multiple partitions, which pay be computed on different nodes of the cluster\n",
    "* Created in 2 ways:\n",
    "    1. Loading an external dataset\n",
    "    2. Distributing a collection of objects (e.g. list or set) in their driver program\n",
    "* RDDs offer 2 types of operations:\n",
    "    1. Transformations: construct a new RDD from a previous one\n",
    "    2. Actions: Compute a result based on an RDD, either return it to the driver program or save it to an external storage system (e.g. HDFS)\n",
    "* Difference: Although you can define new RDDs any time, Spark only computes them in a <i> lazy</i> fashion, the first time they are used in an action\n",
    "* RDDs are by default recomputed each time you run an action on them\n",
    "    - If you would like to reuse RDD in multiple actions, you can ask Spark to <i>persist</i> it using <code>RDD.persist()</code>. to store the RDD contents in memory (partitioned across the machines in your cluster) and reuse them in future actions\n",
    "    - It does not persist by default because in the context of big data if you do not use the RDD, there is no reason to waste storage space when Spark could instead stream through the data once and just compute the result\n",
    "\n",
    "### Example: Filter data that matches a predicate\n",
    "Create a new RDD holding just the strings that contain \"Python\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythonLines Object:  PythonRDD[274] at RDD at PythonRDD.scala:43\n",
      "lines Object:  README_spark.md MapPartitionsRDD[273] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# print 'spark_home: ', spark_home\n",
    "text_file = sc.textFile(\"README_spark.md\")\n",
    "# print 'text_file: ',text_file\n",
    "\n",
    "\n",
    "# lines is an RDD of strings\n",
    "lines = sc.textFile(\"README_spark.md\")\n",
    "# pythonLines is a new RDD holding just the strings that contain \"Python\"\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "\n",
    "print('pythonLines Object: ',pythonLines)\n",
    "print('lines Object: ', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>lines.filter</code> is a <b>Transformation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line in pythonLines RDD:\n",
      " high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "\n",
      "first line in lines RDD:\n",
      " # Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print('first line in pythonLines RDD:\\n', pythonLines.first())\n",
    "print()\n",
    "print('first line in lines RDD:\\n', lines.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of lazy evaluation:\n",
    "* Computations only executed the first time they are in exected in an action\n",
    "* In this example, if Spark were to load and store all the lines in the file at:\n",
    "        lines = sc.textFile(...)\n",
    "  it would waste a lot of storage space, given that we then immediately filter out many lines. \n",
    "* Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result\n",
    "    - In fact, for the </code>first()</code> action, Spark only scans the file until it finds the first matching line; <b>it does not read the whole file</b>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Practical use of persist\n",
    "In practice, you will often use <code>persist</code> to load a subset of yoru data into memory and query it repeatedly. For example, if we knew that we wanted to compute multiple results about the README lines that contain \"Python\", we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of python lines:  3\n",
      "all python lines:  [u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', u'## Interactive Python Shell', u'Alternatively, if you prefer Python, you can use the Python shell:']\n"
     ]
    }
   ],
   "source": [
    "pythonLines.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "print('number of python lines: ',pythonLines.count())\n",
    "print('all python lines: ', pythonLines.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "* Two options:\n",
    "    1. Load external dataset \n",
    "    2. Parallelize a collection in your driver program\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external dataset:  README.md MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:-2\n",
      "parallelize a collection:  ParallelCollectionRDD[10] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "# load external dataset (1):\n",
    "lines = sc.textFile('README.md')\n",
    "print('external dataset: ', lines)\n",
    "\n",
    "# parallelize a collection in your own driver program (2):\n",
    "lines = sc.parallelize(['pandas','i like pandas'])\n",
    "print('parallelize a collection: ', lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations Filter Example\n",
    "* <code>filter</code> does not mutate the existing <code>inputRDD</code> \n",
    "* Instead, it returns a pointer to an entirely new RDD\n",
    "* <code>inputRDD</code> can be used again to search for lines with the word \"warning\" in them\n",
    "* We can then use another transformation - <code>union</code> - to print out the number of lines that contain either \"error\" or \"warning\"\n",
    "    - <code>union</code> is different than filter in that it operates on two RDDs opposed to one\n",
    "    - Transformations can actually operate on any number of input RDDs\n",
    "* As you derive new RDDs from each other using transformation, Spark keeps track of the set of dependencies between different RDDs called <code>lineage graph</code>\n",
    "    - It uses this information to copute each RDD on demand and to recover los data if part of a persistent RDD is lost\n",
    "    \n",
    "<img src =\"figures/lineage_graph.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badLines RDD:\n",
      "line  1 first warning and error\n",
      "line  2 first warning\n",
      "line  3 second warning\n",
      "line  4 first warning and error\n",
      "inputRDD:\n",
      "line  1 first warning\n",
      "line  2 cats\n",
      "line  3 second warning\n",
      "line  4 cats\n",
      "line  5 first warning and error\n",
      "line  6 cats\n",
      "Note: filter operation does not mutate existing inputRDD\n"
     ]
    }
   ],
   "source": [
    "inputRDD = sc.textFile('log.txt')\n",
    "errorsRDD = inputRDD.filter(lambda x: 'error' in x)\n",
    "warningsRDD = inputRDD.filter(lambda x: 'warning' in x)\n",
    "badLinesRDD = errorsRDD.union(warningsRDD)\n",
    "\n",
    "print('badLines RDD:')\n",
    "[print('line ', i+1 , a) for i,a in enumerate(badLinesRDD.collect())]\n",
    "\n",
    "print('inputRDD:')\n",
    "[print('line ', i+1 , a) for i,a in enumerate(inputRDD.collect())]\n",
    "\n",
    "print('Note: filter operation does not mutate existing inputRDD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions Example\n",
    "* Actions <i>do something</i> with our dataset\n",
    "* <code>count()</code>: returns the count as a number\n",
    "* <code>take()</code>: collects a number of elements from the RDD\n",
    "* <code>collect()</code>: Retrieves the entire RDD; useful if your program filters RDDs down to a very small size and you'd like to deal with it locally. \n",
    "    - <b> Your entire dataset must fit in memory on a single machine to use <code>collect()</code></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 4 lines\n",
      "2 Examples: \n",
      "first warning and error\n",
      "first warning\n"
     ]
    }
   ],
   "source": [
    "print('Input has', badLinesRDD.count(), 'lines' )\n",
    "print ('2 Examples: ')\n",
    "for a in badLinesRDD.take(2):\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce vs. Spark Exerpt \n",
    "<i>Spark uses lazy evaluation to reduce the number of passes it has to take over our data\n",
    "by grouping operations together. In MapReduce systems like Hadoop, developers often\n",
    "have to spend a lot time considering how to group together operations to minimize the\n",
    "number of MapReduce passes. In Spark, there is no substantial benefit to writing a single\n",
    "complex map instead of chaining together many simple operations. Thus, users are free\n",
    "to organize their program into smaller, more manageable operations</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Functions to Spark\n",
    "* When you pass a function that is the member of an object (e.g. <code>self.field</code>), Spark sends the <i> entire</i> object to worker nodes, which can be much larger than the bit of information you need\n",
    "* This can also cause your program to fail, if your class contains objects that Python can't figure out how to pickle\n",
    "\n",
    "<b> Don't do this:</b>\n",
    "\n",
    "    class Word Functions(object):\n",
    "        ...\n",
    "        def getMatchesNoReference(self,rdd):\n",
    "            return rdd.filter(lambda x: self.query in x)\n",
    "\n",
    "<b> Do this:</b>\n",
    "\n",
    "    class Word Functions(object):\n",
    "        ...\n",
    "        def getMatchesNoReference(self,rdd):\n",
    "            query = self.query\n",
    "            return rdd.filter(lambda x: query in x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Transformations and Actions\n",
    "* The two most <b>common transformations</b> you will likely be using are <code>map()</code> and <code>filter()</code>\n",
    "    - <code>map()</code> takes in a function and applies it to each element in the RDD with the result of the function being a new value of each element\n",
    "    - <code>filter()</code> takes in a function and returns an RDD that only has elements that pass the <code>filter()</code> function\n",
    "* <code>flatmap()</code> produces multiple output elements for each input element \n",
    "    - Called individually for each element in our input RDD\n",
    "    - Instead of returning a single element, we return an iterator with our return values\n",
    "    - Rather than producing an RDD of iterators, we return an RDD that consists of the elements from all the iterators\n",
    "    \n",
    "   \n",
    "### Pseudo Set Operators \n",
    "* RDDs support many of the operations of mathematical sets, such as union, intersection, even when the RDDs themselves are not properly sets \n",
    "* The set property most frequently missing from our RDDs is the uniqueness of elements\n",
    "    - If we only want unique elements we can use <code>RDD.distinct</code> transformation to produce a new RDD with only distinct items\n",
    "    - Note that <code>distinct()</code> is expensive as it requires shuffling all the data over the network to ensure that we only receive one copy of each element \n",
    "* <code>union()</code>: returns an RDD consisting of data from both sources, but if there are duplicates in the input RDDs, the result will also contain duplicates\n",
    "* <code>intersection()</code>: only returns elements in both RDDs and removes all duplicates (including duplicates from a single RDD)\n",
    "    - Performance of <code>intersection</code> is much worse than <code>union</code> since it requires a shuffle over the network to identify the common elements\n",
    "        - In a shuffle operation we have to send the results to different machines rather than processing them locally\n",
    "* <code>subtract(other)</code> takes in another RDD and returns an RDD that only has values present in teh first RDD and not the second RDD \n",
    "* <code>cartesian(other)</code> transformation results in possible pairs of (a,b) where a is the source RDD and b is the other RDD\n",
    "\n",
    "<img src='figures/cartesian_product_figure.png'>\n",
    "\n",
    "### Examples of Transformations\n",
    "\n",
    "#### Map and Flat Map Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map Example\n",
      "original:  [1, 2, 3, 4]\n",
      "mapped output:  [1, 4, 9, 16]\n",
      "\n",
      "Flatmap Example \n",
      "original:  ['hello world', 'hi']\n",
      "flatmap output:  ['hello', 'world', 'hi']\n"
     ]
    }
   ],
   "source": [
    "# Map Example - square each element\n",
    "print('Map Example')\n",
    "original = [1,2,3,4]\n",
    "nums = sc.parallelize(original)\n",
    "squared = nums.map(lambda x: x*x).collect()\n",
    "print('original: ', original)\n",
    "print('mapped output: ',squared)\n",
    "\n",
    "#Flat Map Example - splitting up an input string into words \n",
    "print('\\nFlatmap Example ')\n",
    "print('original: ', ['hello world','hi'])\n",
    "lines = sc.parallelize(['hello world','hi'])\n",
    "words = lines.flatMap(lambda line: line.split(' '))\n",
    "print('flatmap output: ', words.collect())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map, Sample, Union, Intersection Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map output:  [2, 3, 4, 4]\n",
      "sample output:  [2]\n",
      "rdd1:  [1, 2, 3]\n",
      "rdd2:  [3, 4, 5]\n",
      "union output: [1, 2, 3, 3, 4, 5]\n",
      "intersection output: [3]\n",
      "subtract output: [1, 2]\n",
      "Cartesian product:  [(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 3), (3, 4), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "# map example\n",
    "rdd = sc.parallelize([1,2,3,3])\n",
    "print('map output: ', rdd.map(lambda x: x+1).collect())\n",
    "\n",
    "# sample \n",
    "print('sample output: ', rdd.sample(False,0.5).collect())\n",
    "\n",
    "\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([3,4,5])\n",
    "\n",
    "print('rdd1: ',rdd1.collect())\n",
    "print('rdd2: ', rdd2.collect())\n",
    "\n",
    "# union\n",
    "print('union output:',rdd1.union(rdd2).collect())\n",
    "\n",
    "# intersection\n",
    "print('intersection output:',rdd1.intersection(rdd2).collect())\n",
    "print('subtract output:', rdd1.subtract(rdd2).collect())\n",
    "print('Cartesian product: ',rdd1.cartesian(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "* <code>reduce()</code>: Takes in a function with operates on two elements of the same type of your RDD and returns a new element of the same type \n",
    "    - <code>reduce()</code> is somewhat special. The \"worker function\" for this one must accept two arguments (we've called them x and y here), not just one. The function is called with the first two elements from the list, then with the result of that call and the third element, and so on, until all of the list elements have been handled. This means that our function is called n-1 times if the list contains n elements. The return value of the last call is the result of the reduce() construct. In the above example, it simply adds the arguments, so we get the sum of all elements. \n",
    "* <code>fold()</code>: takes a function with the same signature needed for <code>reduce</code>, but in addition takes a \"zero value\" to be used for the initial call on each partition. \n",
    "    - The zero value you provide should be the identity element for your operation \n",
    "    - That is, applying it multiple times with your function should not change the value (e.g. 0 for +, 1 for *, or an empty list for concatenation)\n",
    "* <code>aggregate()</code>: this function frees us from the constraint of having the return be the same type as the RDD we are working with. <br>We supply:\n",
    "    - An initial value of the type we want to return\n",
    "    - A function to combine the elements from our RDD with the accumulator \n",
    "    - A second function to merge two accumulators, given that each node accumulates its own results locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Fold and Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold output: 12584\n",
      "reduce output: 12584\n"
     ]
    }
   ],
   "source": [
    "L = ['1','25','8','4']\n",
    "print('fold output:',sc.parallelize(L).fold('',lambda a,b:a+b))\n",
    "print('reduce output:',sc.parallelize(L).reduce(lambda a,b:a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the same type output claim\n",
    "<i> Both <code>fold()</code> and <code>reduce()</code> reuqire that the return type of our result be the same type as that of the lements int he RDD we are operating over</i>\n",
    "\n",
    "Note that python does not throw an exception when we typecast the output from int to float, but scala does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT:  0.0416666666667\n"
     ]
    }
   ],
   "source": [
    "# although python does not throw an exception, scala does \n",
    "L = [1,2,3,4]\n",
    "print('OUTPUT: ', sc.parallelize(L).reduce(lambda a,b:a*1./b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Example\n",
    "\n",
    "<code>rdd.aggregate((init_val0,init_val1),combiner_function,merger_function)</code>\n",
    "\n",
    "<code>combiner_function</code>: combine the elements from our RDD with the accumulator<br>\n",
    "<code>merger_function</code>: merge two accumulators, given that each node accumulates its own results locally\n",
    "\n",
    "Note that the number of partitions affects how the elements are combined as shown in the \"intermediate\" step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate: (((0, 0), (3, 2)), (7, 2))\n",
      "sum_count: (10, 4)\n",
      "average: 2.5\n"
     ]
    }
   ],
   "source": [
    "partitions = 2\n",
    "rdd = sc.parallelize([1,2,3,4],partitions)\n",
    "intermediate = rdd.aggregate((0,0),\n",
    "              (lambda x, y: (x[0]+y, x[1] +1)),\n",
    "              (lambda x, y: (x,y)))\n",
    "print('intermediate:',intermediate)\n",
    "sum_count = rdd.aggregate((0,0),\n",
    "              (lambda x, y: (x[0] + y, x[1] +1)),\n",
    "              (lambda x, y: (x[0] + y[0], x[1] +y[1])))\n",
    "print('sum_count:',sum_count)\n",
    "print('average:', sum_count[0]*1./sum_count[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
