{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <code> sc.textFile </code> Quesitons\n",
    "1. **Q**: Is <code>sc.textFile(filepath)</code> a transformation or action?\n",
    "2. **Q**: What does <code>sc.textFile(filepath)</code> return?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <code> sc.textFile </code> Answers\n",
    "1. **A**: Transformation\n",
    "2. **A**: A list of elements where each element is the input file split by newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Java Function Interfaces Questions\n",
    "\n",
    "1. **Q**: Takes in one input and returns one output for use with operations like <code>map()</code> or <code>filter()</code>\n",
    "2. **Q**: Takes in two inputs and returns one output, for use with operations like <code>aggregate()</code> or <code>fold()</code>\n",
    "3. **Q**: Takes in one input and return zero or more outputs for use with operations like <code>flatMap()</code>\n",
    "4. **Q**: Are lambda functions available in Java?\n",
    "5. **Q**: If we want to create a DoubleRDD from an RDD of type T what function would we use?\n",
    "6. **Q**: When we want a DoubleRDD returned from map what function do we use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Java Function Interfaces Answers\n",
    "\n",
    "1. **A**: <code>Function < T,R >  </code>\n",
    "2. **A**: <code>Function2 < T1,T2,R > </code>\n",
    "3. **A**: <code>FlatMapFunction< T,R > </code>\n",
    "4. **A**: Yes, in Java 8 you can also use lambda expressions to implement the function interface i.e.:\n",
    "        RDD <String> errors = lines.filter(s-> s.contains(\"error\")); \n",
    "5. **A**: <code>DoubleFunction< T > </code>\n",
    "6. **A**: <code>mapToDouble()</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which (multiple) transformations are expensive in terms of shuffling data across the network?\n",
    "* take(num)\n",
    "* foreach(func)\n",
    "* distinct()\n",
    "* top(num)\n",
    "* cartesian()\n",
    "* union()\n",
    "* intersection()\n",
    "* sample()\n",
    "* subtract()\n",
    "* repartition()\n",
    "* coalesce() (decreasing the number of partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which (multiple) transformations are expensive in terms of shuffling data across the network?**\n",
    "* take(num): returns n elements from the RDD and attempts to minimize the number of partitions being accessed so it may represent a biased collection\n",
    "* foreach(func): lets us perform computations on each element of the RDD without bringing it back locally\n",
    "* **distinct()**\n",
    "* top(num)\n",
    "* **cartesian()**\n",
    "* union()\n",
    "* **intersection()**\n",
    "* sample()\n",
    "* **subtract()**\n",
    "* **repartition()**\n",
    "* coalesce(): allows avoiding data movement, but only if you are decreasing the number of RDD partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** What happens to variables within the closure that are serialized and sent to each executor? Namely, a counter that is sent and incremented?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Variables within the closure sent to each executor are copies and thus, when the counter is reference within the foreach function, it's no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fact\n",
    "\n",
    "Python all of the functions are implemented on the base RDD class but will fail at runtime if the type of data in the RDD is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Persist Store Type - Unserialized or Serialized?\n",
    "How will the default <code>persist()</code> store the data in the JVM heap?\n",
    "1. Scala\n",
    "2. Java\n",
    "3. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Persist Store Type - Unserialized or Serialized?\n",
    "How will the default <code>persist()</code> store the data in the JVM heap?\n",
    "1. Scala - unserialized objects\n",
    "2. Java - unserialized objects\n",
    "3. Python - we always serialize the data that persist stores; pickled objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Java and Scala (multiple)?\n",
    "* MEMORY_AND_CACHE\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_HDFS\n",
    "* MEMORY_AND_SER\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* MEMORY_AND_PICKLE\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Java and Scala (multiple)?\n",
    "* **MEMORY_AND_CACHE**\n",
    "* MEMORY_ONLY\n",
    "* **MEMORY_AND_HDFS**\n",
    "* **MEMORY_AND_SER**\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* **MEMORY_AND_PICKLE**\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Python?\n",
    "* MEMORY_AND_CACHE\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_HDFS\n",
    "* MEMORY_AND_SER\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* MEMORY_AND_PICKLE\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Python (multiple)?\n",
    "* **MEMORY_AND_CACHE**\n",
    "* MEMORY_ONLY\n",
    "* **MEMORY_AND_HDFS**\n",
    "* **MEMORY_AND_SER**\n",
    "* **MEMORY_ONLY_SER**\n",
    "* MEMORY_AND_DISK\n",
    "* **MEMORY_AND_DISK_SER**\n",
    "* **MEMORY_AND_PICKLE**\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "//given this \n",
    "val someFruits = sc.parallelize(List(\"orange\",\"apple\",\"apricots\"))\n",
    "\n",
    "// what is returned?\n",
    "someFruits.keyBy(x=>x(0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((o,orange), (a,apple), (a,apricots))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//given this \n",
    "val someFruits = sc.parallelize(List(\"orange\",\"apple\",\"apricots\"))\n",
    "\n",
    "// what is returned?\n",
    "someFruits.keyBy(x=>x(0)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the Java tuple type and how are elements referenced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<code>scala.Tuple2 </code> class is used to create tuples i.e.:\n",
    "    New Tuple2(elem1, elem2)\n",
    "    \n",
    "where its elements can be accessed with <code>._1()</code> and <code>._2()</code> methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Count Example in Java\n",
    "\n",
    "    JavaRDD<String> textFile = sc.textFile(\"hdfs://...\");\n",
    "    JavaRDD<String> words = textFile.flatMap(new FlatMapFunction<String, String>() {\n",
    "      public Iterable<String> call(String s) { return Arrays.asList(s.split(\" \")); }\n",
    "    });\n",
    "    JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {\n",
    "      public Tuple2<String, Integer> call(String s) { return new Tuple2<String, Integer>(s, 1); }\n",
    "    });\n",
    "    JavaPairRDD<String, Integer> counts = pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {\n",
    "      public Integer call(Integer a, Integer b) { return a + b; }\n",
    "    });\n",
    "    counts.saveAsTextFile(\"hdfs://...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we check the partition size in:\n",
    "1. Java\n",
    "2. Scala\n",
    "3. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we check the partition size in:\n",
    "1. Java: <code> rdd.partitions.size()</code>\n",
    "2. Scala: <code> rdd.partitions.size()</code>\n",
    "3. Python: <code> rdd.getNumPartitions()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What should you do after an RDD has been transformed by <code>partitionBy</code>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Persisted or Cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which operations automatically result in an RDD with known partitioning information?\n",
    "* union()\n",
    "* join()\n",
    "* leftOuterJoin()\n",
    "* rightOuterJoin()\n",
    "* intersection()\n",
    "* sortByKey()\n",
    "* sort()\n",
    "* grouByKey()\n",
    "* reduceByKey()\n",
    "* combineByKey()\n",
    "* partitionBy()\n",
    "* cogroup()\n",
    "* groupWith() \n",
    "\n",
    "For the following assume the parent has a known partitioner\n",
    "* mapValues() \n",
    "* map()\n",
    "* flatMapValues() \n",
    "* filter() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which operations automatically result in an RDD with known partitioning information?\n",
    "* union()\n",
    "* **join()**\n",
    "* **leftOuterJoin()**\n",
    "* **rightOuterJoin()**\n",
    "* intersection()\n",
    "* **sortByKey()**\n",
    "* **sort()**\n",
    "* ** grouByKey() **\n",
    "* ** reduceByKey()**\n",
    "* **combineByKey()**\n",
    "* **partitionBy()**\n",
    "* **cogroup()**\n",
    "* **groupWith() **\n",
    "\n",
    "\n",
    "\n",
    "For the following assume the parent has a known partitioner\n",
    "* **mapValues() **\n",
    "* map()\n",
    "* ** flatMapValues()** \n",
    "* **filter() **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the difference between s3:// and s3n://?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* s3://: refers to an HDFS file system mapped into a S3 bucket which is sitting on an AWS storage cluster where the files stored by this filesystem can be larger than 5GB, but are not interoperable with other S3 tools\n",
    "* s3n://: refers to a regular file, readable from the outside world as this s3 url and has a 5GB limit on file size imposed by s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
