{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <code> sc.textFile </code> Quesitons\n",
    "1. **Q**: Is <code>sc.textFile(filepath)</code> a transformation or action?\n",
    "2. **Q**: What does <code>sc.textFile(filepath)</code> return?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <code> sc.textFile </code> Answers\n",
    "1. **A**: Transformation\n",
    "2. **A**: A list of elements where each element is the input file split by newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Java Function Interfaces Questions\n",
    "\n",
    "1. **Q**: Takes in one input and returns one output for use with operations like <code>map()</code> or <code>filter()</code>\n",
    "2. **Q**: Takes in two inputs and returns one output, for use with operations like <code>aggregate()</code> or <code>fold()</code>\n",
    "3. **Q**: Takes in one input and return zero or more outputs for use with operations like <code>flatMap()</code>\n",
    "4. **Q**: Are lambda functions available in Java?\n",
    "5. **Q**: If we want to create a DoubleRDD from an RDD of type T what function would we use?\n",
    "6. **Q**: When we want a DoubleRDD returned from map what function do we use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Java Function Interfaces Answers\n",
    "\n",
    "1. **A**: <code>Function < T,R >  </code>\n",
    "2. **A**: <code>Function2 < T1,T2,R > </code>\n",
    "3. **A**: <code>FlatMapFunction< T,R > </code>\n",
    "4. **A**: Yes, in Java 8 you can also use lambda expressions to implement the function interface i.e.:\n",
    "        RDD <String> errors = lines.filter(s-> s.contains(\"error\")); \n",
    "5. **A**: <code>DoubleFunction< T > </code>\n",
    "6. **A**: <code>mapToDouble()</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which (multiple) transformations are expensive in terms of shuffling data across the network?\n",
    "* take(num)\n",
    "* foreach(func)\n",
    "* distinct()\n",
    "* top(num)\n",
    "* cartesian()\n",
    "* union()\n",
    "* intersection()\n",
    "* sample()\n",
    "* subtract()\n",
    "* repartition()\n",
    "* coalesce() (decreasing the number of partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which (multiple) transformations are expensive in terms of shuffling data across the network?\n",
    "* take(num): returns n elements from the RDD and attempts to minimize the number of partitions being accessed so it may represent a biased collection\n",
    "* foreach(func): lets us perform computations on each element of the RDD without bringing it back locally\n",
    "* **distinct()**\n",
    "* top(num)\n",
    "* **cartesian()**\n",
    "* union()\n",
    "* **intersection()**\n",
    "* sample()\n",
    "* **subtract()**\n",
    "* **repartition()**\n",
    "* coalesce(): allows avoiding data movement, but only if you are decreasing the number of RDD partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** What happens to variables within the closure that are serialized and sent to each executor? Namely, a counter that is sent and incremented?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Variables within the closure sent to each executor are copies and thus, when the counter is reference within the foreach function, it's no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fact\n",
    "\n",
    "Python all of the functions are implemented on the base RDD class but will fail at runtime if the type of data in the RDD is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Persist Store Type - Unserialized or Serialized?\n",
    "How will the default <code>persist()</code> store the data in the JVM heap?\n",
    "1. Scala\n",
    "2. Java\n",
    "3. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Persist Store Type - Unserialized or Serialized?\n",
    "How will the default <code>persist()</code> store the data in the JVM heap?\n",
    "1. Scala - unserialized objects\n",
    "2. Java - unserialized objects\n",
    "3. Python - we always serialize the data that persist stores; pickled objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Java and Scala (multiple)?\n",
    "* MEMORY_AND_CACHE\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_HDFS\n",
    "* MEMORY_AND_SER\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* MEMORY_AND_PICKLE\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Java and Scala (multiple)?\n",
    "* **MEMORY_AND_CACHE**\n",
    "* MEMORY_ONLY\n",
    "* **MEMORY_AND_HDFS**\n",
    "* **MEMORY_AND_SER**\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* **MEMORY_AND_PICKLE**\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Python?\n",
    "* MEMORY_AND_CACHE\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_HDFS\n",
    "* MEMORY_AND_SER\n",
    "* MEMORY_ONLY_SER\n",
    "* MEMORY_AND_DISK\n",
    "* MEMORY_AND_DISK_SER\n",
    "* MEMORY_AND_PICKLE\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Storage Levels \n",
    "Which of the following is **not** a storage level for Python (multiple)?\n",
    "* **MEMORY_AND_CACHE**\n",
    "* MEMORY_ONLY\n",
    "* **MEMORY_AND_HDFS**\n",
    "* **MEMORY_AND_SER**\n",
    "* **MEMORY_ONLY_SER**\n",
    "* MEMORY_AND_DISK\n",
    "* **MEMORY_AND_DISK_SER**\n",
    "* **MEMORY_AND_PICKLE**\n",
    "* DISK_ONLY\n",
    "* OFF_HEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "//given this \n",
    "val someFruits = sc.parallelize(List(\"orange\",\"apple\",\"apricots\"))\n",
    "\n",
    "// what is returned?\n",
    "someFruits.keyBy(x=>x(0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((o,orange), (a,apple), (a,apricots))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//given this \n",
    "val someFruits = sc.parallelize(List(\"orange\",\"apple\",\"apricots\"))\n",
    "\n",
    "// what is returned?\n",
    "someFruits.keyBy(x=>x(0)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the Java tuple type and how are elements referenced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<code>scala.Tuple2 </code> class is used to create tuples i.e.:\n",
    "    New Tuple2(elem1, elem2)\n",
    "    \n",
    "where its elements can be accessed with <code>._1()</code> and <code>._2()</code> methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Count Example in Java\n",
    "\n",
    "    JavaRDD<String> textFile = sc.textFile(\"hdfs://...\");\n",
    "    JavaRDD<String> words = textFile.flatMap(new FlatMapFunction<String, String>() {\n",
    "      public Iterable<String> call(String s) { return Arrays.asList(s.split(\" \")); }\n",
    "    });\n",
    "    JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {\n",
    "      public Tuple2<String, Integer> call(String s) { return new Tuple2<String, Integer>(s, 1); }\n",
    "    });\n",
    "    JavaPairRDD<String, Integer> counts = pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {\n",
    "      public Integer call(Integer a, Integer b) { return a + b; }\n",
    "    });\n",
    "    counts.saveAsTextFile(\"hdfs://...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we check the partition size in:\n",
    "1. Java\n",
    "2. Scala\n",
    "3. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we check the partition size in:\n",
    "1. Java: <code> rdd.partitions.size()</code>\n",
    "2. Scala: <code> rdd.partitions.size()</code>\n",
    "3. Python: <code> rdd.getNumPartitions()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What should you do after an RDD has been transformed by <code>partitionBy</code>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Persist or Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which operations automatically result in an RDD with known partitioning information?\n",
    "* union()\n",
    "* join()\n",
    "* leftOuterJoin()\n",
    "* rightOuterJoin()\n",
    "* intersection()\n",
    "* sortByKey()\n",
    "* sort()\n",
    "* groupByKey()\n",
    "* reduceByKey()\n",
    "* combineByKey()\n",
    "* partitionBy()\n",
    "* cogroup()\n",
    "* groupWith() \n",
    "\n",
    "For the following assume the parent has a known partitioner\n",
    "* mapValues() \n",
    "* map()\n",
    "* flatMapValues() \n",
    "* filter() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which operations automatically result in an RDD with known partitioning information?\n",
    "* union()\n",
    "* **join()**\n",
    "* **leftOuterJoin()**\n",
    "* **rightOuterJoin()**\n",
    "* intersection()\n",
    "* **sortByKey()**\n",
    "* **sort()**\n",
    "* ** grouByKey() **\n",
    "* ** reduceByKey()**\n",
    "* **combineByKey()**\n",
    "* **partitionBy()**\n",
    "* **cogroup()**\n",
    "* **groupWith() **\n",
    "\n",
    "\n",
    "\n",
    "For the following assume the parent has a known partitioner\n",
    "* **mapValues() **\n",
    "* map()\n",
    "* ** flatMapValues()** \n",
    "* **filter() **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the difference between s3:// and s3n://?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* s3://: refers to an HDFS file system mapped into a S3 bucket which is sitting on an AWS storage cluster where the files stored by this filesystem can be larger than 5GB, but are not interoperable with other S3 tools\n",
    "* s3n://: refers to a regular file, readable from the outside world as this s3 url and has a 5GB limit on file size imposed by s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accumulator Questions\n",
    "1. How is an accumulator initialized?\n",
    "2. What is the return type?\n",
    "3. From the point of view of the workers what are the permissions (read only, write only, read and write) of the accumulator value?\n",
    "4. If we have an accumulator counter, how do we add to it in Java?\n",
    "5. How do you access the accumulator's value in scala, python and Java?\n",
    "6. if we want a reliable absolute counter, where should it be used?\n",
    "7. What needs to be extended to enable custom accumulators?\n",
    "8. Other than addition, what other operations can accumulators take on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Accumulator Answers\n",
    "1. How is an accumulator initialized?<br> <code>SparkContext.accumulator(initialValue)</code>\n",
    "2. What is the return type?<br><code>org.apache.spark.Accumulator[T]</code> object, where T is the type of <code>initialValue</code>\n",
    "3. From the point of view of the workers what are the permissions (read only, write only, read and write)\n",
    "<br> write-only\n",
    "4. If we have an accumulator counter, how do we add to it in Java?\n",
    "<br> the <code>add</code> method in Java\n",
    "5. How do you access the accumulator's value in scala, python and Java?\n",
    "<br> python & scala: <code>.value</code>; java: <code>.value()</code>\n",
    "6. If we want a reliable absolute counter, where should it be used?\n",
    "<br> Actions - updates the accumulator only once regardless of failures or multiple evals \n",
    "7. What needs to be extended to enable custom accumulators?\n",
    "<br> <code>AccumulatorParam</code>\n",
    "8. Other than addition, what other operations can accumulators take on?\n",
    "<br> any operation provided that the operation is commutative and associative (i.e. max value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "//given:\n",
    "val accum = sc.accumulator(0)\n",
    "sc.parallelize(1 to 4).foreach(accum.add(_))\n",
    "\n",
    "//what is the value of accum?\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//given:\n",
    "val accum = sc.accumulator(0)\n",
    "sc.parallelize(1 to 4).foreach(accum.add(_))\n",
    "\n",
    "//what is the value of accum?\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Broadcast Variables Questions\n",
    "1. How is a broadcast variable initlaized?\n",
    "2. What condition must the type of the broadcast variable satisify?\n",
    "3. How often is the broadcast variable sent to each node? What are the permissions from the point of view of the worker nodes (read only, write only, read and write)? \n",
    "4. How do updates made on the worker nodes affect the value of the broadcast variable on the driver?\n",
    "5. By default, how (often) does spark distribute (non-broadcasted) variables referenced in your closure to the worker nodes? (i.e. if you use the same variable in multiple parallel operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadcast Variables Answers\n",
    "1. How is a broadcast variable initlaized?\n",
    "<br><code>SparkContext.broadcast(variable)</code>\n",
    "2. What condition must the type of the broadcast variable satisify?\n",
    "<br>Must be serializable\n",
    "3. How often is the broadcast variable sent to each node? What are the permissions from the point of view of the worker nodes (read only, write only, read and write)? \n",
    "<br>Broadcast variables are sent to each node only once and are read-only from the POV of the worker nodes.\n",
    "4. How do updates made on the worker nodes impact the value of the broadcast variable on the driver?\n",
    "<br>Updates made on the worker nodes only impact their local copy of the broadcast variable and changes made are not propagated to other nodes.\n",
    "5. By default, how often does spark distribute (non-broadcasted) variables referenced in your closure to the worker nodes (i.e. if you use the same variable in multiple parallel operations)?\n",
    "<br>By default, spark automatically sends all variables referenced in your closures to the worker nodes; if you use the same variable in multiple parallel operations Spark will send it separately for each operation by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "//given\n",
    "val broadcastVar = sc.broadcast((Array(1,2,3,4)))\n",
    "val rdd = sc.parallelize(List(1,1,1,1))\n",
    "\n",
    "// what does the following return?\n",
    "rdd.map(x=> x+ broadcastVar.value(1)).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3, 3, 3, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//given\n",
    "val broadcastVar = sc.broadcast((Array(1,2,3,4)))\n",
    "val rdd = sc.parallelize(List(1,1,1,1))\n",
    "\n",
    "// what does the following return?\n",
    "rdd.map(x=> x+ broadcastVar.value(1)).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which of the following (multiple) are appropriate ways you might try and fix the following exception\n",
    "<code> java.io.NotSerializableException: java.io.ObjectOutputStream</code> \n",
    "\n",
    "a) Change the class causing the problem to a serializable object<br>\n",
    "b) Create a subclass of the type in question that implements Java's Externalizable interface/customize the serialization behavior using Kyro<br>\n",
    "c) Enable the debug info by using the <code>--driver-java-options</code> and <code>--executor-java-options</code> flags with  <code>-Dsun.io.serialization.extended DebugInfo=true</code> to spark-submit <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a,b,c are all appropriate ways you might try and fix the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Execution Questions\n",
    "1. When an action is called on an RDD, what does the Scheduler do and where in the lineage does it start?\n",
    "2. What is a job comprised of and how many of these things are there? (answer this before going on to the next question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Execution Answers\n",
    "1. When an action is called on an RDD, what does the Scheduler do and where in the lineage does it start?\n",
    "<br> The scheduler submits jobs to compute all the needed RDDs. It starts at the final RDD being computd and works backwards to find what it must compute \n",
    "2. What is a job comprised of and how many of these things are there? (answer this before going on to the next question)\n",
    "<br> Jobs are comprised of one or more stages \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Execution Questions - Staging \n",
    "1. What are stages?\n",
    "2. How many RDDs does each stage correspond to (and how)?\n",
    "3. What order are stages processed in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Execution Answers - Staging \n",
    "1. What are stages?\n",
    "<br>Stages are parallel waves of computation composed of tasks\n",
    "2. How many RDDs does each stage correspond to (and how) in the DAG? \n",
    "<br>Each stage corresponds to one or more RDDs in the DAG or a single stage can correspond to multiple RDDs due to pipelining.\n",
    "3. What order are stages processed in?\n",
    "<br>Stages are processed in order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the solution to the following error?\n",
    "    java.net.BindException: Address already in use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solution \n",
    "\n",
    "    --conf spark.ui.port=your_port\n",
    "    \n",
    "or \n",
    "\n",
    "    conf.set(\"spark.ui.port\",your_port) //within script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caching Policy\n",
    "1. What is the default storage level for the <code>cache()</code> operation?\n",
    "2. How does MEMORY_AND_DISK store RDD partitions?\n",
    "3. What are the effects of MEMORY_ONLY_SER and MEMORY_AND_DISK_SER?\n",
    "4. How does the cost of garbage collection scale?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Caching Policy\n",
    "1. What is the default storage level for the <code>cache()</code> operation?\n",
    "<br>MEMORY_ONLY\n",
    "2. How does MEMORY_AND_DISK store RDD partitions?\n",
    "<br>when there is not enough space to cache new RDD partitions old ones will be dropped to disk and simply read back to memory from a local store if they are needed again\n",
    "3. What are the effects of MEMORY_ONLY_SER and MEMORY_AND_DISK_SER?\n",
    "<br>Slightly slow down the cache operation due to the cost of serializing objects but can substantially reduce time spent on garbage collection in the JVM\n",
    "4. How does the cost of garbage collection scale?\n",
    "<br>The cost of garbage collection scales with the number of objects on the heap **not the number of bytes of data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark's architecture allows for what type of scaling?\n",
    "a) sublinear<br>\n",
    "b) linear<br>\n",
    "c) squared<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark's architecture allows for what type of scaling?\n",
    "a) sublinear<br>\n",
    "b) **linear<br>**\n",
    "c) squared<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we read a JSON file and write out a Parquet File?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "// How do we read a JSON file and write out a Parquet file?\n",
    "val df = sqlContext.jsonFile(\"path/to/json/file\")\n",
    "df.saveAsParquetFile(\"path/to/parquet/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you register a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "// how do you register a table?\n",
    "schemaRDD.registerTable(\"table name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Streaming Questions\n",
    "\n",
    "1. How often might you set up checkpointing?\n",
    "2. What is a Dstream?\n",
    "3. What do Transformations yield?\n",
    "4. What do Output Operations do?\n",
    "5. How do you create a Dstream based on data received on a particular port on a local machine (in scala/python, and Java?)\n",
    "6. When should checkpointing be enabled?\n",
    "7. How do we enable checkpointing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Streaming Questions\n",
    "\n",
    "1. How often might you set up checkpointing?\n",
    "<br> 5-10 batches of data\n",
    "2. What is a Dstream?\n",
    "<br> Sequence of data arriving over time where each Dstream is represented as a sequence of RDDs arriving at each time step\n",
    "3. What do Transformations yield?\n",
    "<br> Transformations yield new Dstreams\n",
    "4. What do Output Operations do?\n",
    "<br> Output operations write data to external sources\n",
    "5. How do you create a Dstream based on data received on a particular port on a local machine (in scala/python, and Java?)\n",
    "<br> python/scala: <code>StreamingContext(conf,batchsize).socketTextStream(\"localhost\",portValue)</code>\n",
    "<br> java: <code> JavaStreamingContext jssc = new JavaStreamingContext(conf,batchsize)</code>\n",
    "6. What type of transformations should checkpointing be enabled for?\n",
    "<br> Stateful transformations for fault tolerance\n",
    "7. How do we enable checkpointing?\n",
    "<br> StreamingContext.checkpoint(\"hdfs://\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which (multiple) are stateful transformations?\n",
    "1. map()\n",
    "2. flatMap()\n",
    "3. filter()\n",
    "4. updateStateByKey()\n",
    "5. countByWindow()\n",
    "6. groupByKey()\n",
    "7. transform()\n",
    "8. countByValueAndWindow()\n",
    "9. reduceByWindow()\n",
    "10. reduceByKeyAndWindow()\n",
    "11. reduceByKey()\n",
    "12. transformWith()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which (multiple) are stateful transformations?\n",
    "\n",
    "1. map()\n",
    "2. flatMap()\n",
    "3. filter()\n",
    "4. **updateStateByKey()**\n",
    "5. **countByWindow()**\n",
    "6. groupByKey()\n",
    "7. transform()\n",
    "8. **countByValueAndWindow()**\n",
    "9. **reduceByWindow()**\n",
    "10. **reduceByKeyAndWindow()**\n",
    "11. reduceByKey()\n",
    "12. transformWith()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<code>val inputsStream = ssc.socketStream(...)</code>\n",
    "\n",
    "This inputStream will generate RDDs every 2 seconds, containing last 2 second of data. Now say we define a few window operation on this. The window operation is defined as <code>DStream.window([window duration], [slide duration])</code>. Which is invalid (just one)\n",
    "\n",
    "a) <code>val windowStream1 = inputStream.window(Seconds(4))</code><br>\n",
    "b) <code>val windowStream4 = inputStream.window(Seconds(10), Seconds(10)</code><br>\n",
    "c) <code>val windowStream5 = inputStream.window(Seconds(2), Seconds(2))</code> <br>\n",
    "d) <code>val windowStream6 = inputStream.window(Seconds(11), Seconds(2))</code>  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<code>val inputsStream = ssc.socketStream(...)</code>\n",
    "\n",
    "This inputStream will generate RDDs every 2 seconds, containing last 2 second of data. Now say we define a few window operation on this. The window operation is defined as <code>DStream.window([window duration], [slide duration])</code>. Which is invalid (just one)\n",
    "\n",
    "a) <code>val windowStream1 = inputStream.window(Seconds(4))</code><br>\n",
    "b) <code>val windowStream4 = inputStream.window(Seconds(10), Seconds(10)</code><br>\n",
    "c) <code>val windowStream5 = inputStream.window(Seconds(2), Seconds(2))</code> <br>\n",
    "**d) <code>val windowStream6 = inputStream.window(Seconds(11), Seconds(2))</code>  <br>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which of the following operations do not acccept numTasks as a parameter  and what does numTasks control?\n",
    "\n",
    "1. map()\n",
    "2. flatMap()\n",
    "3. filter()\n",
    "4. reduce()\n",
    "5. mapPartitions()\n",
    "6. All actions\n",
    "7. cartesian()\n",
    "8. pipe()\n",
    "9. sample()\n",
    "10. union()\n",
    "11. intersection()\n",
    "12. join()\n",
    "13. mapPartitionsWithIndex()\n",
    "14. sortByKeys()\n",
    "15. arregateByKey()\n",
    "16. reduceByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which of the following operations do not acccept numTasks as a parameter  and what does numTasks control?\n",
    "\n",
    "numTasks is the argument that controls the number of reducers\n",
    "\n",
    "1. **map()**\n",
    "2. **flatMap()**\n",
    "3. **filter()**\n",
    "4. **reduce()**\n",
    "5. **mapPartitions()**\n",
    "6. **All Actions**\n",
    "7. **cartesian()**\n",
    "8. **pipe()**\n",
    "9. **sample()**\n",
    "10. **union()**\n",
    "11. **intersection()**\n",
    "12. join()\n",
    "13. **mapPartitionsWithIndex()**\n",
    "14. sortByKeys()\n",
    "15. arregateByKey()\n",
    "16. reduceByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    " \n",
    "case class Peep(name: String, age: Int)\n",
    " \n",
    "val vertexArray = Array(\n",
    "  (1L, Peep(\"Kim\", 23)),\n",
    "  (2L, Peep(\"Pat\", 31)),\n",
    "  (3L, Peep(\"Chris\", 52)),\n",
    "  (4L, Peep(\"Kelly\", 39)),\n",
    "  (5L, Peep(\"Leslie\", 45))\n",
    "  )\n",
    "val edgeArray = Array(\n",
    "  Edge(2L, 1L, 7),\n",
    "  Edge(2L, 4L, 2),\n",
    "  Edge(3L, 2L, 4),\n",
    "  Edge(3L, 5L, 3),\n",
    "  Edge(4L, 1L, 1),\n",
    "  Edge(5L, 3L, 9)\n",
    "  )\n",
    " \n",
    "val vertexRDD: RDD[(Long, Peep)] = sc.parallelize(vertexArray)\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)\n",
    "val g: Graph[Peep, Int] = Graph(vertexRDD, edgeRDD)\n",
    " \n",
    "val results = g.triplets.filter(t => t.attr > 7)\n",
    " \n",
    "for (triplet <- results.collect) {\n",
    "  println(s\"${triplet.srcAttr.name} loves ${triplet.dstAttr.name}\")\n",
    "}\n",
    "\n",
    "// what is the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leslie loves Chris\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    " \n",
    "case class Peep(name: String, age: Int)\n",
    " \n",
    "val vertexArray = Array(\n",
    "  (1L, Peep(\"Kim\", 23)),\n",
    "  (2L, Peep(\"Pat\", 31)),\n",
    "  (3L, Peep(\"Chris\", 52)),\n",
    "  (4L, Peep(\"Kelly\", 39)),\n",
    "  (5L, Peep(\"Leslie\", 45))\n",
    "  )\n",
    "val edgeArray = Array(\n",
    "  Edge(2L, 1L, 7),\n",
    "  Edge(2L, 4L, 2),\n",
    "  Edge(3L, 2L, 4),\n",
    "  Edge(3L, 5L, 3),\n",
    "  Edge(4L, 1L, 1),\n",
    "  Edge(5L, 3L, 9)\n",
    "  )\n",
    " \n",
    "val vertexRDD: RDD[(Long, Peep)] = sc.parallelize(vertexArray)\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)\n",
    "val g: Graph[Peep, Int] = Graph(vertexRDD, edgeRDD)\n",
    " \n",
    "val results = g.triplets.filter(t => t.attr > 7)\n",
    " \n",
    "for (triplet <- results.collect) {\n",
    "  println(s\"${triplet.srcAttr.name} loves ${triplet.dstAttr.name}\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// Example\n",
    "//reduce by key and window example with inverse function\n",
    "reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])\n",
    "val ipDStream = accessLogsDStream.map(logEntry => (logEntry.getIpAddress(), 1))\n",
    "val ipCountDStream = ipDStream.reduceByKeyAndWindow(\n",
    "  {(x, y) => x + y}, // Adding elements in the new batches entering the window\n",
    "  {(x, y) => x - y}, // Removing elements from the oldest batches exiting the window\n",
    "  Seconds(30),       // Window duration\n",
    "  Seconds(10))       // Slide duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// Another Example\n",
    "//reduce by key and window example w/o inverse function\n",
    "val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(30), Seconds(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For user defined functions, what needs to be imported (java)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <code>import org.apache.spark.sql.api.java.UDF[N]</code>\n",
    "\n",
    "where N: number of parameters in the user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "//example - create a table and register it \n",
    "import org.apache.spark.sql.{SQLContext, Row}\n",
    "case class HappyPerson(handle: String, favouriteBeverage: String)\n",
    "// Create a person and turn it into a Schema RDD\n",
    "val happyPeopleRDD = sc.parallelize(List(HappyPerson(\"holden\", \"coffee\")))\n",
    "val df = sqlContext.createDataFrame(happyPeopleRDD)\n",
    "df.registerTempTable(\"happy_people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you check the partitioner in scala?\n",
    "    import org.apache.spark.HashPartitioner\n",
    "    val userData = sc.sequenceFile[UserID,UserInfo](\"hdfs://\").partitionBy(new HashPartitioner(100)).persist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do you check the partitioner in scala? - Solution\n",
    "    import org.apache.spark.HashPartitioner\n",
    "    val userData = sc.sequenceFile[UserID,UserInfo](\"hdfs://\").partitionBy(new HashPartitioner(100)).persist()\n",
    "    \n",
    "    userData.partitioner\n",
    "    >>res1: Option[spark.Partitioner] = Some(spark.HashPartitioner@5147788d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### By default, how much space is reserved for RDD storage? shuffle memory storage? user code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RDD storage: 60%\n",
    "* Shuffle memory storage: 20%\n",
    "* user code: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,CompactBuffer(2)), (3,CompactBuffer(4, 6)))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd1 = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "val rdd2 = sc.parallelize(Array((3,5)))\n",
    "rdd1.rightOuterjoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((3,(Some(4),5)), (3,(Some(6),5)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd1 = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "val rdd2 = sc.parallelize(Array((3,5)))\n",
    "rdd1.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.flatMapValues(1 to _).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((3,1), (3,2), (3,3), (3,4), (1,1), (1,2), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.flatMapValues(1 to _).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.lookup(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedArray(4, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd = sc.parallelize(Array((3,4),(1,2),(3,6)))\n",
    "rdd.lookup(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd1 = sc.parallelize(List(1,2,3))\n",
    "val rdd2 = sc.parallelize(List(3,4,5))\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 3, 4, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd1 = sc.parallelize(List(1,2,3))\n",
    "val rdd2 = sc.parallelize(List(3,4,5))\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd =  sc.parallelize(List(\"apple\",\"oranges\"))\n",
    "rdd.flatMap(_.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(A, P, P, L, E, O, R, A, N, G, E, S)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd =  sc.parallelize(List(\"apple\",\"oranges\"))\n",
    "rdd.flatMap(_.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// what is the result?\n",
    "val rdd =  sc.parallelize(List(\"apple\",\"oranges\"))\n",
    "rdd.map(_.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(APPLE, ORANGES)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// what is the result?\n",
    "val rdd =  sc.parallelize(List(\"apple\",\"oranges\"))\n",
    "rdd.map(_.toUpperCase).collect()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
