{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs\n",
    "## RDD Basics\n",
    "* RDD: An immutable distributed collection of objects. Each split into multiple partitions, which pay be computed on different nodes of the cluster\n",
    "* Created in 2 ways:\n",
    "    1. Loading an external dataset\n",
    "    2. Distributing a collection of objects (e.g. list or set) in their driver program\n",
    "* RDDs offer 2 types of operations:\n",
    "    1. Transformations: construct a new RDD from a previous one\n",
    "    2. Actions: Compute a result based on an RDD, either return it to the driver program or save it to an external storage system (e.g. HDFS)\n",
    "* Difference: Although you can define new RDDs any time, Spark only computes them in a <i> lazy</i> fashion, the first time they are used in an action\n",
    "* RDDs are by default recomputed each time you run an action on them\n",
    "    - If you would like to reuse RDD in multiple actions, you can ask Spark to <i>persist</i> it using <code>RDD.persist()</code>. to store the RDD contents in memory (partitioned across the machines in your cluster) and reuse them in future actions\n",
    "    - It does not persist by default because in the context of big data if you do not use the RDD, there is no reason to waste storage space when Spark could instead stream through the data once and just compute the result\n",
    "\n",
    "### Example: Filter data that matches a predicate\n",
    "Create a new RDD holding just the strings that contain \"Python\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythonLines Object:  PythonRDD[4] at RDD at PythonRDD.scala:43\n",
      "lines Object:  /home/jo/spark/spark-1.6.1-bin-hadoop2.4/README.md MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# print 'spark_home: ', spark_home\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "# print 'text_file: ',text_file\n",
    "\n",
    "\n",
    "# lines is an RDD of strings\n",
    "lines = sc.textFile(spark_home + \"/README.md\")\n",
    "# pythonLines is a new RDD holding just the strings that contain \"Python\"\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "\n",
    "print 'pythonLines Object: ',pythonLines\n",
    "print 'lines Object: ', lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>lines.filter</code> is a <b>Transformation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line in pythonLines RDD:\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "\n",
      "first line in lines RDD:\n",
      "# Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print 'first line in pythonLines RDD:\\n', pythonLines.first()\n",
    "print \n",
    "print 'first line in lines RDD:\\n', lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of lazy evaluation:\n",
    "* Computations only executed the first time they are in exected in an action\n",
    "* In this example, if Spark were to load and store all the lines in the file at:\n",
    "        lines = sc.textFile(...)\n",
    "  it would waste a lot of storage space, given that we then immediately filter out many lines. \n",
    "* Instead, once Spark sees the whole chain of transformations, it can compute just the data needed for its result\n",
    "    - In fact, for the </code>first()</code> action, Spark only scans the file until it finds the first matching line; <b>it does not read the whole file</b>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Practical use of persist\n",
    "In practice, you will often use <code>persist</code> to load a subset of yoru data into memory and query it repeatedly. For example, if we knew that we wanted to compute multiple results about the README lines that contain \"Python\", we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of python lines:  3\n",
      "all python lines:  [u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', u'## Interactive Python Shell', u'Alternatively, if you prefer Python, you can use the Python shell:']\n"
     ]
    }
   ],
   "source": [
    "pythonLines.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "print 'number of python lines: ',pythonLines.count()\n",
    "print 'all python lines: ', pythonLines.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "* Two options:\n",
    "    1. Load external dataset \n",
    "    2. Parallelize a collection in your driver program\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external dataset:  /home/jo/spark/spark-1.6.1-bin-hadoop2.4/README.md MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:-2\n",
      "parallelize a collection:  ParallelCollectionRDD[10] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "# load external dataset (1):\n",
    "lines = sc.textFile(spark_home + '/README.md')\n",
    "print 'external dataset: ', lines\n",
    "\n",
    "# parallelize a collection in your own driver program (2):\n",
    "lines = sc.parallelize(['pandas','i like pandas'])\n",
    "print 'parallelize a collection: ', lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations Filter Example\n",
    "* <code>filter</code> does not mutate the existing <code>inputRDD</code> \n",
    "* Instead, it returns a pointer to an entirely new RDD\n",
    "* <code>inputRDD</code> can be used again to search for lines with the word \"warning\" in them\n",
    "* We can then use another transformation - <code>union</code> - to print out the number of lines that contain either \"error\" or \"warning\"\n",
    "    - <code>union</code> is different than filter in that it operates on two RDDs opposed to one\n",
    "    - Transformations can actually operate on any number of input RDDs\n",
    "* As you derive new RDDs from each other using transformation, Spark keeps track of the set of dependencies between different RDDs called <code>lineage graph</code>\n",
    "    - It uses this information to copute each RDD on demand and to recover los data if part of a persistent RDD is lost\n",
    "    \n",
    "<img src =\"lineage_graph.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badLines RDD:\n",
      "line  1 first warning and error\n",
      "line  2 first warning\n",
      "line  3 second warning\n",
      "line  4 first warning and error\n",
      "inputRDD:\n",
      "line  1 first warning\n",
      "line  2 cats\n",
      "line  3 second warning\n",
      "line  4 cats\n",
      "line  5 first warning and error\n",
      "line  6 cats\n",
      "Note: filter operation does not mutate existing inputRDD\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "inputRDD = sc.textFile('log.txt')\n",
    "errorsRDD = inputRDD.filter(lambda x: 'error' in x)\n",
    "warningsRDD = inputRDD.filter(lambda x: 'warning' in x)\n",
    "badLinesRDD = errorsRDD.union(warningsRDD)\n",
    "\n",
    "print('badLines RDD:')\n",
    "[print('line ', i+1 , a) for i,a in enumerate(badLinesRDD.collect())]\n",
    "\n",
    "print('inputRDD:')\n",
    "[print('line ', i+1 , a) for i,a in enumerate(inputRDD.collect())]\n",
    "\n",
    "print('Note: filter operation does not mutate existing inputRDD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions Example\n",
    "* Actions <i>do something</i> with our dataset\n",
    "* <code>count()</code>: returns the count as a number\n",
    "* <code>take()</code>: collects a number of elements from the RDD\n",
    "* <code>collect()</code>: Retrieves the entire RDD; useful if your program filters RDDs down to a very small size and you'd like to deal with it locally. <b> Your entire dataset must fit in memory on a single machine to use <code>collect()</code></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 4 lines\n",
      "2 Examples: \n",
      "first warning and error\n",
      "first warning\n"
     ]
    }
   ],
   "source": [
    "print('Input has', badLinesRDD.count(), 'lines' )\n",
    "print ('2 Examples: ')\n",
    "for a in badLinesRDD.take(2):\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce vs. Spark Exerpt \n",
    "<i>Spark uses lazy evaluation to reduce the number of passes it has to take over our data\n",
    "by grouping operations together. In MapReduce systems like Hadoop, developers often\n",
    "have to spend a lot time considering how to group together operations to minimize the\n",
    "number of MapReduce passes. In Spark, there is no substantial benefit to writing a single\n",
    "complex map instead of chaining together many simple operations. Thus, users are free\n",
    "to organize their program into smaller, more manageable operations</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Functions to Spark\n",
    "...tbc pg 42 (in pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Example\n",
    "\n",
    "The word count script below is quite simple. It takes the following steps:\n",
    "\n",
    "1. Split each line from the file into words\n",
    "2. Map each word to a tuple containing the word and an initial count of 1\n",
    "3. Sum up the count for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'Scala,', 1),\n",
       " (u'environment', 1),\n",
       " (u'only', 1),\n",
       " (u'rich', 1),\n",
       " (u'Apache', 1),\n",
       " (u'sc.parallelize(range(1000)).count()', 1),\n",
       " (u'Building', 1),\n",
       " (u'guide,', 1),\n",
       " (u'return', 2),\n",
       " (u'Please', 3),\n",
       " (u'Try', 1),\n",
       " (u'not', 1),\n",
       " (u'Spark', 13),\n",
       " (u'scala>', 1),\n",
       " (u'Note', 1),\n",
       " (u'cluster.', 1),\n",
       " (u'./bin/pyspark', 1),\n",
       " (u'params', 1),\n",
       " (u'through', 1),\n",
       " (u'GraphX', 1),\n",
       " (u'[run', 1),\n",
       " (u'abbreviated', 1),\n",
       " (u'[project', 2),\n",
       " (u'##', 8),\n",
       " (u'library', 1),\n",
       " (u'see', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'[Apache', 1),\n",
       " (u'will', 1),\n",
       " (u'#', 1),\n",
       " (u'processing,', 1),\n",
       " (u'for', 11),\n",
       " (u'[building', 1),\n",
       " (u'provides', 1),\n",
       " (u'print', 1),\n",
       " (u'supports', 2),\n",
       " (u'built,', 1),\n",
       " (u'[params]`.', 1),\n",
       " (u'available', 1),\n",
       " (u'run', 7),\n",
       " (u'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       "  1),\n",
       " (u'This', 2),\n",
       " (u'Hadoop,', 2),\n",
       " (u'Tests', 1),\n",
       " (u'example:', 1),\n",
       " (u'-DskipTests', 1),\n",
       " (u'Maven](http://maven.apache.org/).', 1),\n",
       " (u'programming', 1),\n",
       " (u'running', 1),\n",
       " (u'against', 1),\n",
       " (u'site,', 1),\n",
       " (u'comes', 1),\n",
       " (u'package.', 1),\n",
       " (u'and', 10),\n",
       " (u'package.)', 1),\n",
       " (u'prefer', 1),\n",
       " (u'documentation,', 1),\n",
       " (u'submit', 1),\n",
       " (u'tools', 1),\n",
       " (u'use', 3),\n",
       " (u'from', 1),\n",
       " (u'For', 2),\n",
       " (u'./bin/run-example', 2),\n",
       " (u'fast', 1),\n",
       " (u'systems.', 1),\n",
       " (u'<http://spark.apache.org/>', 1),\n",
       " (u'Hadoop-supported', 1),\n",
       " (u'way', 1),\n",
       " (u'README', 1),\n",
       " (u'MASTER', 1),\n",
       " (u'engine', 1),\n",
       " (u'building', 2),\n",
       " (u'usage', 1),\n",
       " (u'instance:', 1),\n",
       " (u'with', 3),\n",
       " (u'protocols', 1),\n",
       " (u'And', 1),\n",
       " (u'this', 1),\n",
       " (u'setup', 1),\n",
       " (u'shell:', 2),\n",
       " (u'project', 1),\n",
       " (u'following', 2),\n",
       " (u'distribution', 1),\n",
       " (u'detailed', 2),\n",
       " (u'have', 1),\n",
       " (u'stream', 1),\n",
       " (u'is', 6),\n",
       " (u'higher-level', 1),\n",
       " (u'tests', 2),\n",
       " (u'1000:', 2),\n",
       " (u'sample', 1),\n",
       " (u'[\"Specifying', 1),\n",
       " (u'Alternatively,', 1),\n",
       " (u'file', 1),\n",
       " (u'need', 1),\n",
       " (u'You', 3),\n",
       " (u'instructions.', 1),\n",
       " (u'different', 1),\n",
       " (u'programs,', 1),\n",
       " (u'storage', 1),\n",
       " (u'same', 1),\n",
       " (u'machine', 1),\n",
       " (u'Running', 1),\n",
       " (u'which', 2),\n",
       " (u'you', 4),\n",
       " (u'A', 1),\n",
       " (u'About', 1),\n",
       " (u'sc.parallelize(1', 1),\n",
       " (u'locally.', 1),\n",
       " (u'Hive', 2),\n",
       " (u'optimized', 1),\n",
       " (u'uses', 1),\n",
       " (u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "  1),\n",
       " (u'variable', 1),\n",
       " (u'The', 1),\n",
       " (u'data', 1),\n",
       " (u'a', 8),\n",
       " (u'\"yarn\"', 1),\n",
       " (u'Thriftserver', 1),\n",
       " (u'processing.', 1),\n",
       " (u'./bin/spark-shell', 1),\n",
       " (u'Python', 2),\n",
       " (u'Spark](#building-spark).', 1),\n",
       " (u'clean', 1),\n",
       " (u'the', 21),\n",
       " (u'requires', 1),\n",
       " (u'talk', 1),\n",
       " (u'help', 1),\n",
       " (u'Hadoop', 3),\n",
       " (u'high-level', 1),\n",
       " (u'find', 1),\n",
       " (u'web', 1),\n",
       " (u'Shell', 2),\n",
       " (u'how', 2),\n",
       " (u'graph', 1),\n",
       " (u'run:', 1),\n",
       " (u'should', 2),\n",
       " (u'to', 14),\n",
       " (u'module,', 1),\n",
       " (u'given.', 1),\n",
       " (u'directory.', 1),\n",
       " (u'must', 1),\n",
       " (u'SparkPi', 2),\n",
       " (u'do', 2),\n",
       " (u'Programs', 1),\n",
       " (u'Many', 1),\n",
       " (u'YARN,', 1),\n",
       " (u'using', 2),\n",
       " (u'Example', 1),\n",
       " (u'Once', 1),\n",
       " (u'HDFS', 1),\n",
       " (u'Because', 1),\n",
       " (u'name', 1),\n",
       " (u'Testing', 1),\n",
       " (u'refer', 2),\n",
       " (u'Streaming', 1),\n",
       " (u'SQL', 2),\n",
       " (u'them,', 1),\n",
       " (u'analysis.', 1),\n",
       " (u'set', 2),\n",
       " (u'Scala', 2),\n",
       " (u'thread,', 1),\n",
       " (u'individual', 1),\n",
       " (u'examples', 2),\n",
       " (u'changed', 1),\n",
       " (u'runs.', 1),\n",
       " (u'Pi', 1),\n",
       " (u'More', 1),\n",
       " (u'Python,', 2),\n",
       " (u'Versions', 1),\n",
       " (u'its', 1),\n",
       " (u'version', 1),\n",
       " (u'wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1),\n",
       " (u'`./bin/run-example', 1),\n",
       " (u'Configuration', 1),\n",
       " (u'command,', 2),\n",
       " (u'can', 6),\n",
       " (u'core', 1),\n",
       " (u'Guide](http://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " (u'MASTER=spark://host:7077', 1),\n",
       " (u'Documentation', 1),\n",
       " (u'downloaded', 1),\n",
       " (u'distributions.', 1),\n",
       " (u'Spark.', 1),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " (u'[\"Building', 1),\n",
       " (u'`examples`', 2),\n",
       " (u'on', 5),\n",
       " (u'package', 1),\n",
       " (u'of', 5),\n",
       " (u'APIs', 1),\n",
       " (u'pre-built', 1),\n",
       " (u'Big', 1),\n",
       " (u'or', 3),\n",
       " (u'learning,', 1),\n",
       " (u'locally', 2),\n",
       " (u'overview', 1),\n",
       " (u'one', 2),\n",
       " (u'(You', 1),\n",
       " (u'Online', 1),\n",
       " (u'versions', 1),\n",
       " (u'your', 1),\n",
       " (u'threads.', 1),\n",
       " (u'>>>', 1),\n",
       " (u'spark://', 1),\n",
       " (u'contains', 1),\n",
       " (u'system', 1),\n",
       " (u'start', 1),\n",
       " (u'build/mvn', 1),\n",
       " (u'basic', 1),\n",
       " (u'configure', 1),\n",
       " (u'that', 2),\n",
       " (u'N', 1),\n",
       " (u'\"local[N]\"', 1),\n",
       " (u'DataFrames,', 1),\n",
       " (u'particular', 2),\n",
       " (u'be', 2),\n",
       " (u'an', 3),\n",
       " (u'easiest', 1),\n",
       " (u'Interactive', 2),\n",
       " (u'cluster', 2),\n",
       " (u'page](http://spark.apache.org/documentation.html)', 1),\n",
       " (u'<class>', 1),\n",
       " (u'example', 3),\n",
       " (u'are', 1),\n",
       " (u'Data.', 1),\n",
       " (u'mesos://', 1),\n",
       " (u'computing', 1),\n",
       " (u'URL,', 1),\n",
       " (u'in', 5),\n",
       " (u'general', 2),\n",
       " (u'To', 2),\n",
       " (u'at', 2),\n",
       " (u'1000).count()', 1),\n",
       " (u'if', 4),\n",
       " (u'built', 1),\n",
       " (u'no', 1),\n",
       " (u'Java,', 1),\n",
       " (u'MLlib', 1),\n",
       " (u'also', 4),\n",
       " (u'other', 1),\n",
       " (u'build', 3),\n",
       " (u'online', 1),\n",
       " (u'several', 1),\n",
       " (u'[Configuration', 1),\n",
       " (u'class', 2),\n",
       " (u'programs', 2),\n",
       " (u'documentation', 3),\n",
       " (u'It', 2),\n",
       " (u'graphs', 1),\n",
       " (u'./dev/run-tests', 1),\n",
       " (u'first', 1),\n",
       " (u'latest', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
